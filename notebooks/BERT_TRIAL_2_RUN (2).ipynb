{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers pytorch-lightning"
      ],
      "metadata": {
        "id": "fmM3fHGEJA3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "wkVjmnyMJJHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1je_AW1I1OT"
      },
      "outputs": [],
      "source": [
        "import warnings \n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from typing import List \n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "from torch.utils.data import DataLoader, Dataset "
      ],
      "metadata": {
        "id": "P4avsIKXJRDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import pytorch_lightning as pl"
      ],
      "metadata": {
        "id": "9ZshHXepJZw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configs\n",
        "\n",
        "EPOCHS=3\n",
        "DEVICE='cuda'\n",
        "\n",
        "TRAIN_BATCH_SIZE=2\n",
        "VALID_BATCH_SIZE=2\n",
        "TEST_BATCH_SIZE=1\n",
        "\n",
        "ACCUMULATION_STEPS=4\n",
        "MAX_LEN=128\n",
        "\n",
        "TRAINING_DATASET='/content/train.csv'\n",
        "TESTING_DATASET='/content/evaluation.csv'\n",
        "CHECKPOINT='/content/checkpoints'\n",
        "\n",
        "TOKENIZER=transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "BERTMODEL=transformers.BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "aRRdb9R8Jldy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bert Dataset Approach 1 \n",
        "\n",
        "class BERTDataset:\n",
        "    def __init__(self, texts : List[str], reasons : List[str], targets : List[int]):\n",
        "        self.texts=texts\n",
        "        self.reasons=reasons\n",
        "        self.targets=targets\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, item : int):\n",
        "        text=str(self.texts[item])\n",
        "        reason=str(self.reasons[item])\n",
        "\n",
        "        text=\" \".join(text.split())\n",
        "        reason=\" \".join(reason.split()) \n",
        "        inputs=TOKENIZER.encode_plus(\n",
        "            text, \n",
        "            reason, \n",
        "            add_special_tokens=True, \n",
        "            max_length=MAX_LEN,\n",
        "            padding='max_length'\n",
        "        )\n",
        "\n",
        "        ids=inputs['input_ids']\n",
        "        token_types=inputs['token_type_ids']\n",
        "        mask=inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids' : torch.tensor(ids, dtype=torch.long), \n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids' : torch.tensor(token_types, dtype=torch.long),\n",
        "            'targets' : torch.tensor(int(self.targets[item]), dtype=torch.float32)\n",
        "        }"
      ],
      "metadata": {
        "id": "BcbVCh2lKc1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utility functions \n",
        "\n",
        "def train_valid_split(df, test_split=0.2):\n",
        "    train_length = int(len(df) * (1 - test_split))\n",
        "    train_data = pd.DataFrame(df.iloc[:train_length, :])\n",
        "    valid_data = pd.DataFrame(df.iloc[train_length:, :])\n",
        "    return (train_data, valid_data)\n",
        "\n",
        "def get_text_reason_target(df):\n",
        "    return [\n",
        "        df['text'].tolist(),\n",
        "        df['reason'].tolist(),\n",
        "        df['label'].tolist()\n",
        "    ]\n",
        "\n",
        "def bce_loss(outputs, targets):\n",
        "    return nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "\n",
        "def get_train_valid_test_dataset(train_df, valid_df, test_df, loaders=True):\n",
        "    train_text, train_reason, train_targets = get_text_reason_target(train_df) \n",
        "    valid_text, valid_reason, valid_targets = get_text_reason_target(valid_df)\n",
        "    test_text, test_reason, test_targets = get_text_reason_target(test_df)\n",
        "\n",
        "    train_bert_dataset = BERTDataset(\n",
        "    texts=train_text, reasons=train_reason, targets=train_targets\n",
        "    )\n",
        "\n",
        "    valid_bert_dataset = BERTDataset(\n",
        "        texts=valid_text, reasons=valid_reason, targets=valid_targets\n",
        "    )\n",
        "\n",
        "    test_bert_dataset = BERTDataset(\n",
        "        texts=test_text, reasons=test_reason, targets=test_targets\n",
        "    )\n",
        "\n",
        "    if loaders:\n",
        "        return [\n",
        "            DataLoader(train_bert_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True),\n",
        "            DataLoader(valid_bert_dataset, batch_size=VALID_BATCH_SIZE),\n",
        "            DataLoader(test_bert_dataset, batch_size=TEST_BATCH_SIZE)\n",
        "        ]\n",
        "    \n",
        "    return [\n",
        "        train_bert_dataset, valid_bert_dataset, test_bert_dataset\n",
        "    ]"
      ],
      "metadata": {
        "id": "JLNrpQ7dKE37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(TRAINING_DATASET)\n",
        "test_dataset = pd.read_csv(TESTING_DATASET)\n",
        "\n",
        "train_dataset, valid_dataset = train_valid_split(dataset)"
      ],
      "metadata": {
        "id": "LxUkc58JKjgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, valid_loader, test_loader = get_train_valid_test_dataset(\n",
        "    train_dataset, valid_dataset, test_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "sBC5yGVXKsDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertBaseUncasedSingleSentence(nn.Module):\n",
        "    def __init__(self, dropout_prob=0.2):\n",
        "        super(BertBaseUncasedSingleSentence, self).__init__() \n",
        "        self.bert=BERTMODEL.to(DEVICE)\n",
        "        self.bert_drop=nn.Dropout(dropout_prob)\n",
        "        self.classifier=nn.Linear(768, 1)\n",
        "    \n",
        "    def forward(self, ids : torch.Tensor, token_type_ids : torch.Tensor, mask : torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Computes the similarity between two sentences provided using the dataset format \n",
        "\n",
        "        Args:\n",
        "            ids (torch.Tensor): Token ids to be used\n",
        "            token_type_ids (torch.Tensor): Token type ids to be used\n",
        "            mask (torch.Tensor): Attention mask\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Returns logits between 0 to 1 for computing the probability of similarity\n",
        "        \"\"\"\n",
        "        ids = ids.to(DEVICE)\n",
        "        token_type_ids = token_type_ids.to(DEVICE)\n",
        "        mask=mask.to(DEVICE)\n",
        "        output = self.bert(\n",
        "            ids,token_type_ids,mask\n",
        "        )\n",
        "\n",
        "        embeddings = self.bert_drop(output.pooler_output)\n",
        "        return self.classifier(embeddings)"
      ],
      "metadata": {
        "id": "563YTsmENWpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(logits, targets, threshold=0.5, conversion=True):\n",
        "    if conversion:\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        targets = targets.cpu().numpy()\n",
        "\n",
        "    predictions = np.array([0 if logit < threshold else 1 for logit in logits])\n",
        "    return np.sum(predictions==targets) / len(logits)\n",
        "\n",
        "\n",
        "def free_gpu(ids, mask, token_type_ids, targets, logits):\n",
        "    del ids \n",
        "    del mask  \n",
        "    del token_type_ids\n",
        "    del targets\n",
        "    del logits\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "VLuRSVYVTW2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Batch loop\n",
        "\n",
        "def train_batch(model, dataloader, optimizer, show_after_every_batch=100):\n",
        "    batch_loss = 0.0\n",
        "    accuracy = 0.0 \n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        ids, mask, token_type_ids, targets = batch.values() \n",
        "        ids = ids.to(DEVICE)\n",
        "        mask = mask.to(DEVICE)\n",
        "        token_type_ids = token_type_ids.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits=model(ids, mask, token_type_ids).view(-1)\n",
        "        loss = bce_loss(logits, targets)\n",
        "        batch_loss += loss.item()\n",
        "        predictions = torch.sigmoid(logits)\n",
        "        accuracy += compute_accuracy(predictions, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch_idx + 1) % show_after_every_batch == 0:\n",
        "            print(f'TRAIN: After completing batch {batch_idx + 1} | batch_loss : {(batch_loss / batch_idx):.3f} | batch_accuracy: {(accuracy / batch_idx):.3f}')\n",
        "        \n",
        "        free_gpu(ids, mask, token_type_ids, targets, logits)\n",
        "    return model, batch_loss / len(dataloader), accuracy / len(dataloader)\n",
        "\n",
        "\n",
        "# Validation Batch loop\n",
        "\n",
        "def validate_batch(model, dataloader, show_after_every_batch=10):\n",
        "    batch_loss = 0.0\n",
        "    accuracy = 0.0 \n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            ids, mask, token_type_ids, targets = batch.values() \n",
        "            ids = ids.to(DEVICE)\n",
        "            mask = mask.to(DEVICE)\n",
        "            token_type_ids = token_type_ids.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "\n",
        "            logits=model(ids, mask, token_type_ids).view(-1)\n",
        "            loss = bce_loss(logits, targets)\n",
        "            predictions = torch.sigmoid(logits)\n",
        "            accuracy += compute_accuracy(predictions, targets)\n",
        "\n",
        "            if (batch_idx + 1) % show_after_every_batch == 0:\n",
        "                print(\n",
        "                    f'EVAL: After completing batch {batch_idx + 1} | batch_loss : {(batch_loss / batch_idx):.3f} | batch_accuracy: {(accuracy / batch_idx):.3f}'\n",
        "            )\n",
        "            \n",
        "            free_gpu(ids, mask, token_type_ids, targets, logits)\n",
        "    return batch_loss / len(dataloader), accuracy / len(dataloader)"
      ],
      "metadata": {
        "id": "L7onnOkYOjKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertBaseUncasedSingleSentence().to(DEVICE)"
      ],
      "metadata": {
        "id": "2HStrHqgOgiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "6go3xCpkYJrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    print(\"\\nTRAINING\\n\")\n",
        "    model, train_batch_loss, train_batch_acc = train_batch(model, train_loader, optimizer)\n",
        "    print(\"\\nVALIDATION\\n\")\n",
        "    valid_batch_loss, valid_batch_acc = validate_batch(model, valid_loader, 100)\n",
        "    print(\"\\nEVALUATION\\n\")\n",
        "    test_batch_loss, test_batch_acc = validate_batch(model, test_loader, 1000)\n",
        "\n",
        "    print(\"\\n=========== Epochs Results ===========\")\n",
        "    print(f\"TRAIN: Loss : {train_batch_loss:.3f}, Acc: {train_batch_acc:.3f}\")\n",
        "    print(f\"VALID: Loss : {valid_batch_loss:.3f}, Acc: {valid_batch_acc:.3f}\")\n",
        "    print(f\"TEST: Loss : {test_batch_loss:.3f}, Acc: {test_batch_acc:.3f}\")\n",
        "    print(\"=======================================\\n\")"
      ],
      "metadata": {
        "id": "LkPp5TSrY-NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yb11Na3SnZB7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}