# -*- coding: utf-8 -*-
"""All Examples Simase BERT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mFEUM0qi4heB_HZ6UQFKfo_ukMvTBOlH
"""

# pip install transformers wordcloud plotly nltk spacy

import os
import warnings 
import numpy as np  
import pandas as pd
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore')

plt.rcParams['font.size'] = 18
plt.rcParams['figure.figsize'] = (20,8)

import re 
import nltk
import spacy 
import seaborn as sns 
import plotly.express as px

from typing import List, Dict 
from tqdm.notebook import tqdm 
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from plotly.offline import init_notebook_mode
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

tqdm.pandas()
nltk.download('omw-1.4')
nltk.download('stopwords')
lemm = WordNetLemmatizer()

sns.set_style("darkgrid")
init_notebook_mode(connected=True)
spacy_eng = spacy.load("en_core_web_sm")

from google.colab import files  
files.upload()

def load_dfs(names : List):
    dfs = []
    for name in names:
        df = pd.read_csv(name)
        df.name = name 
        dfs.append(df)
    return dfs

def train_valid_split(df, test_split=0.2):
    train_length = int(len(df) * (1 - test_split))
    train_data = pd.DataFrame(df.iloc[:train_length, :])
    valid_data = pd.DataFrame(df.iloc[train_length:, :])
    return (train_data, valid_data)

test_df, only_positives, adverserial, generated = load_dfs(
    ['evaluation.csv', 
    'train.csv', 
    'adverserial.csv', 
    'generated_data.csv']
)

def text_cleaning(x):
    questions = re.sub('\s+\n+', ' ', x)
    questions = re.sub('[^a-zA-Z0-9]', ' ', questions)
    questions = questions.lower()
    return questions

def to_string(x): return str(x)

def text_clean_df_cols(dfs : List, cols : List):
    for df in dfs:
        print(f"=> For df: {str(df.name)}")
        df = pd.DataFrame(df)
        for col in cols:
            try:
                df[col] = df[col].progress_apply(to_string)
                df[col] = df[col].progress_apply(text_cleaning)
            except:
                continue 
    print("Finished")

text_clean_df_cols(
    [test_df, only_positives, adverserial, generated], 
    ['text', 'reason']
)

# merge all the training samples generated

merged = pd.concat([only_positives, adverserial, generated], axis=0)
merged = merged.sample(frac=1)

merged = merged.drop(['Unnamed: 0'], axis=1).reset_index(drop=True)

train_df, valid_df = train_valid_split(merged, 0.1)

from transformers import (AutoTokenizer,
                          DataCollatorWithPadding,
                          TFAutoModel,DistilBertConfig,
                          TFDistilBertModel, 
                          BertConfig, TFBertModel, TFRobertaModel)

model_checkpoint = 'bert-base-uncased'
TOKENIZER = AutoTokenizer.from_pretrained(model_checkpoint)

def encode_text(text, tokenizer):
    encoded = tokenizer.batch_encode_plus(
        text,
        add_special_tokens=True,
        max_length=50,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors="tf",
    )

    input_ids = np.array(encoded["input_ids"], dtype="int32")
    attention_masks = np.array(encoded["attention_mask"], dtype="int32")

    return {
        "input_ids": input_ids,
        "attention_masks": attention_masks
    }

def get_encoded_items(df):
    return [
        encode_text(df['text'].tolist(), TOKENIZER), 
        encode_text(df['reason'].tolist(), TOKENIZER),
        df['label'].tolist()
    ]

import tensorflow as tf
from tensorflow.keras.layers import (Embedding, 
                                     Layer, 
                                     Dense, 
                                     Dropout, 
                                     MultiHeadAttention, 
                                     LayerNormalization, 
                                     Input, GlobalAveragePooling1D)
from tensorflow.keras.models import Sequential, Model


class L1Dist(Layer):
    def __init__(self,**kwargs):
        super().__init__()
        
    def call(self,embedding1,embedding2):
        return tf.math.abs(embedding1 - embedding2)

# stratergy 

strategy = tf.distribute.get_strategy()
BATCH_SIZE=16

with strategy.scope():
    transformer_model = TFBertModel.from_pretrained(model_checkpoint)

    input_ids_in1 = Input(shape=(None,),name='input_ids1', dtype='int32')
    input_masks_in1 = Input(shape=(None,), name='attention_mask1', dtype='int32')
    input_ids_in2 = Input(shape=(None,),name='input_ids2', dtype='int32')
    input_masks_in2 = Input(shape=(None,), name='attention_mask2', dtype='int32')

    embedding_layer1 = transformer_model(input_ids_in1, attention_mask=input_masks_in1).last_hidden_state
    embedding_layer2 = transformer_model(input_ids_in2, attention_mask=input_masks_in2).last_hidden_state

    embedding1 = GlobalAveragePooling1D()(embedding_layer1)
    embedding2 = GlobalAveragePooling1D()(embedding_layer2)
    l1_dist = L1Dist()(embedding1,embedding2)

    x = Dense(512, activation='relu')(l1_dist)
    output = Dense(1, activation='sigmoid')(x)

    model = Model(inputs=[input_ids_in1, input_masks_in1, input_ids_in2, input_masks_in2], outputs = output)
    model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),metrics='accuracy')

for layer in model.layers[:5]:
    layer.trainable = False

model.summary()

from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

earlystopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)

learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', 
                                            patience=3, 
                                            verbose=1, 
                                            factor=0.3, 
                                            min_lr=0.00000001)

train_text, train_reason, train_label = get_encoded_items(train_df)
valid_text, valid_reason, valid_label = get_encoded_items(valid_df)
test_text, test_reason, test_label = get_encoded_items(test_df)

X1_train , X2_train, y_train = train_text, train_reason, np.asarray(train_label)
X1_val , X2_val, y_val = test_text, test_reason, np.asarray(test_label)

history = model.fit(
    (
        np.asarray(X1_train["input_ids"]),
        np.asarray(X1_train["attention_masks"]),
        np.asarray(X2_train["input_ids"]),
        np.asarray(X2_train["attention_masks"]),
    ),
    y_train,
    batch_size=BATCH_SIZE,
    epochs=5,
    validation_data=(
        (
            np.asarray(X1_val["input_ids"]),
            np.asarray(X1_val["attention_masks"]),
            np.asarray(X2_val["input_ids"]),
            np.asarray(X2_val["attention_masks"]),
        ),
        y_val,
    ),
    callbacks=[earlystopping, learning_rate_reduction],
)

